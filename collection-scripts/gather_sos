#!/bin/bash
# Gather SOS reports from the OpenShift nodes that are running OpenStack pods.
# They are stored uncompressed in the must-gather so there is no nested
# compression of sos reports within the must-gather.
#   SOS_SERVICES: comma separated list of services to gather SOS reports from,
#                 empty string skips sos report gathering.  eg: cinder,glance
#                 Defaults to all of them.
#   SOS_ONLY_PLUGINS: list of sos report plugins to use. Empty string to run
#                     them all. Defaults to: block,cifs,crio,devicemapper,
#                     devices,iscsi,lvm2,memory,multipath,nfs,nis,nvme,podman,
#                     process,processor,selinux,scsi,udev,openstack_edpm
#   SOS_DECOMPRESS: bool to disable decompressing sos reports. Set to 0 to disable
#                   or set to 1 to enable. Defaults to 1
#
# TODO: Confirm with GSS the default for SOS_ONLY_PLUGINS

# When called from the shell directly
if [[ -z "$DIR_NAME" ]]; then
    CALLED=1
    DIR_NAME=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    source "${DIR_NAME}/common.sh"
fi

###############################################################################
# VARIABLE INITIALIZATION

# If unset use all default services
if [ "${SOS_SERVICES-unset}" = "unset" ]; then
    SOS_SERVICES=( "${OSP_SERVICES[@]}" )

# If list of services is set and empty, nothing to do
elif [[ -z "${SOS_SERVICES[*]}" ]]; then
    echo "Skipping SOS gathering for controller nodes"
    [[ $CALLED -eq 1 ]] && exit 0
    return

# If set, convert to an array
else
    IFS=',' read -r -a SOS_SERVICES <<< "$SOS_SERVICES"
fi

# Default to some plugins if SOS_ONLY_PLUGINS is not set
SOS_ONLY_PLUGINS="${SOS_ONLY_PLUGINS-block,cifs,crio,devicemapper,devices,iscsi,lvm2,memory,multipath,nfs,nis,nvme,podman,process,processor,selinux,scsi,udev,openstack_edpm}"
if [[ -n "$SOS_ONLY_PLUGINS" ]]; then
    SOS_LIMIT="--only-plugins $SOS_ONLY_PLUGINS"
fi

# This option is used for CI purposes and
# is enabled by default
SOS_DECOMPRESS=${SOS_DECOMPRESS:-1}

SOS_PATH="${BASE_COLLECTION_PATH}/sos-reports"
SOS_PATH_NODES="${BASE_COLLECTION_PATH}/sos-reports/_all_nodes"

TMPDIR=/var/tmp/sos-osp

###############################################################################
# SOS GATHERING

gather_node_sos () {
    node="$1"
    echo "Generating SOS Report for ${node}"
    # Can only run 1 must-gather at a time on each host. We remove any existing
    # toolbox container running from previous time with `podman rm`
    #
    # Current toolbox can ask via stdin if we want to update [1] so we just
    # update the container beforehand to prevent it from ever asking. In the
    # next toolbox [2] that may no longer be the case.
    # [1]: https://github.com/coreos/toolbox/blob/9a7c840fb4881f406287bf29e5f35b6625c7b358/rhcos-toolbox#L37
    # [2]: https://github.com/coreos/toolbox/issues/60
    oc debug "node/$node" -- chroot /host bash \
      -c "echo 'TOOLBOX_NAME=toolbox-osp' > /root/.toolboxrc ; \
          rm -rf \"${TMPDIR}\" && \
          mkdir -p \"${TMPDIR}\" && \
          sudo podman rm --force toolbox-osp;  \
          sudo --preserve-env podman pull --authfile /var/lib/kubelet/config.json registry.redhat.io/rhel9/support-tools && \
          toolbox sos report --batch $SOS_LIMIT --tmp-dir=\"${TMPDIR}\""

    # shellcheck disable=SC2181
    if [ $? -ne 0 ]; then
        echo "Failed to run sos report on node ${node}, won't retrieve data"
        return 1
    fi

    # Wait until we know for sure the debug pod is gone, this is to try to
    # workaround the following failure:
    #   Error from server (BadRequest): container "container-00" in pod debug is waiting to start: ContainerCreating
    while oc get pod "${node}-debug" 1>/dev/null 2>&1 1>/dev/; do
        sleep 1
    done
    sleep 1

    # Download and optionally decompress the tar.xz file from the remote node into the
    # must-gather directory.
    # Not decompressing at this stage outside of a CI environment would
    # require changes be made to the yank tool
    echo "Retrieving SOS Report for ${node}"
    mkdir "${SOS_PATH_NODES}/sosreport-$node"
    # Add "--loglevel 6" to help debug in case there's a failure
    oc debug --loglevel 6 "node/$node" -- bash -c "cat \"/host${TMPDIR}/\"*.tar.xz" | tee "${SOS_PATH_NODES}/sosreport-$node.tar.xz"

    # shellcheck disable=SC2181
    if [ $? -ne 0 ]; then
        echo "Failed to download sosreport-$node.tar.xz not deleting file"
        return 1
    fi

    # if were decompressing the sos report, remove the
    # original sos archive
    if [[ ${SOS_DECOMPRESS} -eq 1 ]]; then
        tar --one-top-level="${SOS_PATH_NODES}/sosreport-$node" --strip-components=1 --exclude='*/dev/null' -Jxf ${SOS_PATH_NODES}/sosreport-$node.tar.xz
        rm "${SOS_PATH_NODES}/sosreport-$node.tar.xz"
    fi

    # Ensure write access to the sos reports directories so must-gather rsync doesn't fail
    chmod +w -R "${SOS_PATH_NODES}/sosreport-$node/"

    sleep 1
    # Delete the tar.xz file from the remote node
    oc debug "node/$node" -- chroot /host bash -c "rm -rf $TMPDIR"
}


###############################################################################
# MAIN
#
dest_svc_path () {
    local svc=$1
    # Some services have the project name in the service label, eg: cinder
    if [[ "${SOS_SERVICES[*]}" == *"${svc}"* ]]; then
        echo -n "${SOS_PATH}/${svc}"
        return
    fi

    # Other services have the component in the service label, eg: nova-api
    for os_svc in "${SOS_SERVICES[@]}"; do
        if [[ "$svc" == "$os_svc"* ]]; then
            echo -n "${SOS_PATH}/${os_svc}"
            return
        fi
    done
}

mkdir -p "${SOS_PATH_NODES}"

# Get list of nodes and service label for each of the OpenStack service pods
# Not using -o jsonpath='{.spec.nodeName}' because it uses space separator
svc_nodes=$(/usr/bin/oc -n openstack get pod -l service --no-headers -o=custom-columns=NODE:.spec.nodeName,SVC:.metadata.labels.service,NAME:.metadata.name)
nodes=''
while read -r node svc name; do
    svc_path=$(dest_svc_path "$svc")
    if [[ -n "$svc_path" ]]; then
        nodes="${nodes}${node}"$'\n'
        mkdir -p "$svc_path"
        sos_dir="${svc_path}/sos-report-${name}"
        [[ ! -e "$sos_dir" ]] && ln -s "../_all_nodes/sosreport-$node" "$sos_dir" 2>/dev/null
    fi
done <<< "$svc_nodes"

# Remove duplicated nodes because they are running multiple services
nodes=$(echo "$nodes" | sort | uniq)
echo "Will retrieve SOS reports from nodes ${nodes//$'\n'/ }"

for node in $nodes; do
    [[ -z "$node" ]] && continue
    # Gather SOS report for the node in background
    run_bg gather_node_sos "$node"
done

[[ $CALLED -eq 1 ]] && wait_bg
